{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN to Dintinguish Between Horses and Humans\n",
    "\n",
    "In this section we’ll explore a more complex scenario than the Fashion MNISTclassifier. We’ll extend what we’ve learned about convolutions and convolutional neural\n",
    "networks to try to classify the contents of images where the location of a feature isn’t always in the same place. I’ve created the Horses or Humans dataset for this\n",
    "purpose. </br>\n",
    "\n",
    "**The Horses vs Humans Dataset**\n",
    "\n",
    "The dataset for this section contains over a thousand 300 × 300-pixel images, approximately half each of horses and humans, rendered in different poses. You can see some examples in Figure.</br>\n",
    "\n",
    "![](Horses&Humans.png)\n",
    "\n",
    "As you can see, the subjects have different orientations and poses and the image composition varies. Consider the two horses, for example—their heads are oriented differently,\n",
    "and one is zoomed out showing the complete animal while the other is zoomed in, showing just the head and part of the body. Similarly, the humans are lit differently, have different skin tones, and are posed differently. The man has his hands on his hips, while the woman has hers outstretched. The images also contain backgrounds such as trees and beaches, so a classifier will have to determine which parts of the image are the important features that determine what makes a horse a horse and a human a human, without being affected by the background.</br>\n",
    "\n",
    "While the previous examples of predicting Y = 2X – 1 or classifying small monochrome images of clothing might have been possible with traditional coding, it’s clear that this is far more difficult, and you are crossing the line into where machine learning is essential to solve a problem.</br>\n",
    "\n",
    "An interesting side note is that these images are all computer-generated. The theory is that features spotted in a CGI image of a horse should apply to a real image. You’ll see how well this works later in this chapter.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for downloading and unzipping the training data\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()\n",
    "# End of unzip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the above code\n",
    "This simply downloads the ZIP of the training data and unzips it into a directory at horse-or-human/training (we’ll deal with downloading the validation data shortly). This is the parent directory that will contain subdirectories for the image types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the code for CNN Architecture for Horses and Humans\n",
    "model = tf.keras.model.Sequ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of above code\n",
    "\n",
    "To use the *ImageDataGenerator* we now simply use the above code:\n",
    "\n",
    "We first create an instance of an ImageDataGenerator called train_datagen. We then specify that this will generate images for the training process by flowing them from a\n",
    "directory. The directory is training_dir, as specified earlier. We also indicate some hyperparameters about the data, such as the target size—in this case the images are\n",
    "300 × 300, and the class mode is binary. The mode is usually binary if there are just two types of images (as in this case) or categorical if there are more than two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture for Horses and Humans\n",
    "\n",
    "There are several major differences between this dataset and the Fashion MNIST onethat you have to take into account when designing an architecture for classifying the\n",
    "images. First, the images are much larger—300 × 300 pixels—so more layers may be needed. Second, the images are full color, not grayscale, so each image will have three\n",
    "channels instead of one. Third, there are only two image types, so we have a binary classifier that can be implemented using just a single output neuron, where it\n",
    "approaches 0 for one class and 1 for the other. Keep these considerations in mind when exploring this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the above code\n",
    "There are a number of things to note here. First of all, this is the very first layer. We’re defining 16 filters, each 3 × 3, but the input shape of the image is (300, 300, 3). Remember that this is because our input image is 300 × 300 and it’s in color, so there are three channels, instead of just one for the monochrome Fashion MNIST dataset\n",
    "we were using earlier. </br>\n",
    "\n",
    "At the other end, notice that there’s only one neuron in the output layer. This is because we’re using a binary classifier, and we can get a binary classification with just\n",
    "a single neuron if we activate it with a sigmoid function. The purpose of the sigmoid function is to drive one set of values toward 0 and the other toward 1, which is perfect\n",
    "for binary classification. </br>\n",
    "\n",
    "Next, notice how we stack several more convolutional layers. We do this because our image source is quite large, and we want, over time, to have many smaller images,\n",
    "each with features highlighted. If we take a look at the results of model.summary we’ll see this in action:\n",
    "\n",
    "```\n",
    "Found 1027 images belonging to 2 classes.\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
    "                                                                 \n",
    " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
    "                                                                 \n",
    " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 3136)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 512)               1606144   \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 1)                 513       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 1,704,097\n",
    "Trainable params: 1,704,097\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "Note how, by the time the data has gone through all the convolutional and pooling layers, it ends up as 7 × 7 items. The theory is that these will be activated feature maps\n",
    "that are relatively simple, containing just 49 pixels. These feature maps can then be passed to the dense neural network to match them to the appropriate labels. </br>\n",
    "\n",
    "This, of course, leads us to have many more parameters than the previous network, so it will be slower to train. With this architecture, we’re going to learn 1.7 million\n",
    "parameters. </br>\n",
    "\n",
    "To train the network, we’ll have to compile it with a loss function and an optimizer. In this case the loss function can be binary cross entropy loss function binary cross\n",
    "entropy, because there are only two classes, and as the name suggests this is a loss function that is designed for that scenario. And we can try a new optimizer, root mean\n",
    "square propagation (RMSprop), that takes a learning rate (lr) parameter that allows us to tweak the learning. Here’s the code: </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.7955 - accuracy: 0.5901\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.4327 - accuracy: 0.8286\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.2816 - accuracy: 0.9260\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.1887 - accuracy: 0.9552\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.1358 - accuracy: 0.9649\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.0909 - accuracy: 0.9698\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.0755 - accuracy: 0.9834\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.1505 - accuracy: 0.9737\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0343 - accuracy: 0.9893\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 5.3613e-04 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.2908 - accuracy: 0.9776\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 41s 1s/step - loss: 0.1162 - accuracy: 0.9776\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 2.6936e-04 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 8.7605e-05 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 41s 1s/step - loss: 0.4179 - accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with loss fuction\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the model using fil_generator and \n",
    "# passing it the training_generator we created erlier.\n",
    "history = model.fit(train_generator, epochs=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the above code:\n",
    "This sample will work in Colab, but if you want to run it on your own machine,please ensure that the Pillow libraries are installed using ```pip install pillow```.</br>\n",
    "\n",
    "Also in my case I needed to install scipy, which stands for Scientific Python. It provides more utility functions for optimization, stats and signal processing. use ```pip install scipy``` to install the package.</br>\n",
    "\n",
    "Note that you can't use ```model.fit_generator``` in the latest versions of tensorflow. It is deprecated. Instead you must use ```model.fit``` method.</br>\n",
    "\n",
    "Also in order to use RMSprop as optimizer, there should be few changes to be made in the latest versions of tensorflow. You have to use ```tf.keras.optimizers.RMSprop``` syntax along with ```learning_rate``` parameter as ```lr``` is deprecated in latest versions. </br>\n",
    "\n",
    "Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy on the training set. Of course, this is just with the training data, and isn’t an indication of performance on data that the network hasn’t previously seen. </br>\n",
    "\n",
    "Next we’ll look at adding the validation set using a generator and measuring its performance to give us a good indication of how this model might perform in real life. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Validation to the Horses or Humans Dataset\n",
    "To add validation, you’ll need a validation dataset that’s separate from the training one. In some cases you’ll get a master dataset that you have to split yourself, but in the case of Horses or Humans, there’s a separate validation set that you can download. </br>\n",
    "\n",
    "You can use very similar code to that used for the training images to download the validation set and unzip it into a different directory: </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# # url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# # file_name = \"horse-or-human.zip\"\n",
    "# training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# # urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# # zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# # zip_ref.extractall(training_dir)\n",
    "# # zip_ref.close()\n",
    "# #End of unzip code\n",
    "\n",
    "# # This is the image generator code\n",
    "# import tensorflow as tf\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # All images will be rescaled by 1./255\n",
    "# train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     training_dir,\n",
    "#     target_size = (300, 300),\n",
    "#     class_mode = 'binary'\n",
    "# )\n",
    "\n",
    "# # This is the CNN architecture for Horses and Humans\n",
    "# model = tf.keras.models.Sequential([        \n",
    "#     tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compiling the model with loss fuction and optimizer\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#             optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "#             metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Training the model using fil_generator and \n",
    "# # passing it the training_generator we created erlier.\n",
    "# history = model.fit(train_generator, epochs=15)\n",
    "\n",
    "# This is the code for downloading validation dataset to a new directory\n",
    "validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
    "\n",
    "validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/validation'\n",
    "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
    "zip_ref.extractall(validation_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the validation data, you can set up another ```ImageDataGenerator``` to manage these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with loss fuction and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the model using fil_generator and \n",
    "# passing it the training_generator we created erlier.\n",
    "history = model.fit(train_generator, epochs=15)\n",
    "\n",
    "# This is the code for downloading validation dataset to a new directory\n",
    "# validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
    "\n",
    "# validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/validation'\n",
    "# urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
    "# zip_ref.extractall(validation_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the code for managing validation images\n",
    "validation_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                    validation_dir,\n",
    "                    target_size=(300, 300),\n",
    "                    class_mode='binary'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have TensorFlow perform the validation for you, you simply update your ```model.fit``` method to indicate that you want to use the validation data to\n",
    "test the model epoch by epoch. You do this by using the validation_data parameter and passing it the validation generator you just constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.5812 - accuracy: 0.7128 - val_loss: 1.0845 - val_accuracy: 0.8359\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.2549 - accuracy: 0.9017 - val_loss: 2.4873 - val_accuracy: 0.7344\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 50s 1s/step - loss: 0.1475 - accuracy: 0.9474 - val_loss: 2.5305 - val_accuracy: 0.7383\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0897 - accuracy: 0.9679 - val_loss: 1.3976 - val_accuracy: 0.7891\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.1187 - accuracy: 0.9825 - val_loss: 3.8713 - val_accuracy: 0.6680\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0165 - accuracy: 0.9951 - val_loss: 1.0871 - val_accuracy: 0.8750\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.1933 - accuracy: 0.9766 - val_loss: 3.0487 - val_accuracy: 0.7773\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0426 - accuracy: 0.9893 - val_loss: 3.0068 - val_accuracy: 0.7773\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 46s 1s/step - loss: 6.9125e-04 - accuracy: 1.0000 - val_loss: 3.4499 - val_accuracy: 0.7852\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 46s 1s/step - loss: 1.1519 - accuracy: 0.9776 - val_loss: 1.3563 - val_accuracy: 0.7383\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 1.8470 - val_accuracy: 0.7812\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.1044 - accuracy: 0.9844 - val_loss: 1.2707 - val_accuracy: 0.7188\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0139 - accuracy: 0.9961 - val_loss: 3.0392 - val_accuracy: 0.7734\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 46s 1s/step - loss: 8.9429e-05 - accuracy: 1.0000 - val_loss: 3.8885 - val_accuracy: 0.7617\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.2837 - accuracy: 0.9766 - val_loss: 1.4423 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with loss fuction and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# This is the code for downloading validation dataset to a new directory\n",
    "# validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
    "\n",
    "# validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/validation'\n",
    "# urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
    "# zip_ref.extractall(validation_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the code for managing validation images\n",
    "validation_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                    validation_dir,\n",
    "                    target_size=(300, 300),\n",
    "                    class_mode='binary'\n",
    "                    )\n",
    "\n",
    "# Training the model using by passing it the training_generator and\n",
    "# validation data.\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=15,\n",
    "                    validation_data=validation_generator\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for 15 epochs, you should see that your model is 98%+ accurate on the training set, but only about 76% on the validation set. This is an indication that the\n",
    "model is overfitting, as we saw in the previous chapter. </br>\n",
    "\n",
    "Still, the performance isn’t bad considering how few images it was trained on, and how diverse those images were. You’re beginning to hit a wall caused by lack of data,\n",
    "but there are some techniques that you can use to improve your model’s performance. We’ll explore them later in this chapter, but before that let’s take a look at how to use\n",
    "this model. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Image Augmentation\n",
    "In the previous section, you built a horse-or-human classifier model that was trained on a relatively small dataset. As a result, you soon began to hit problems classifying\n",
    "some previously unseen images, such as the miscategorization of a woman with a horse because the training set didn’t include any images of people in that pose. </br>\n",
    "\n",
    "One way to deal with such problems is with image augmentation. The idea behind this technique is that, as TensorFlow is loading your data, it can create additional new\n",
    "data by amending what it has using a number of transforms. you can broaden the training set with a variety of other transformations, including: </br>\n",
    "* Rotation\n",
    "* Shifting horizontally\n",
    "* Shifting vertically\n",
    "* Shearing\n",
    "* Zooming\n",
    "* Flipping </br>\n",
    "\n",
    "Because you’ve been using the ImageDataGenerator to load the images, you’ve seen it do a transform already—when it normalized the images like this: </br>\n",
    "```train_datagen = ImageDataGenerator(rescale=1/255)```</br>\n",
    "The other transforms are easily available within the ImageDataGenerator too, so, we will add a few properties in the ```train_datagen``` variable in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "# Also adding image augmantation properties\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                rescale = 1/255,\n",
    "                                rotation_range=40,\n",
    "                                width_shift_range=0.2,\n",
    "                                height_shift_range=0.2,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode='nearest'\n",
    "                            )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with loss fuction and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# This is the code for downloading validation dataset to a new directory\n",
    "# validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
    "\n",
    "# validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/validation'\n",
    "# urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
    "# zip_ref.extractall(validation_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# This is the code for managing validation images\n",
    "validation_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                    validation_dir,\n",
    "                    target_size=(300, 300),\n",
    "                    class_mode='binary'\n",
    "                    )\n",
    "\n",
    "# Training the model using by passing it the training_generator and\n",
    "# validation data.\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=15,\n",
    "                    validation_data=validation_generator\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as well as rescaling the image to normalize it, you’re also doing the following: </br>\n",
    "* Rotating each image randomly up to 40 degrees left or right\n",
    "* Translating the image up to 20% vertically or horizontally\n",
    "* Shearing the image by up to 20%\n",
    "* Zooming the image by up to 20%\n",
    "* Randomly flipping the image horizontally or vertically\n",
    "* Filling in any missing pixels after a move or shear with nearest neighbors </br>\n",
    "\n",
    "When you retrain with these parameters, one of the first things you’ll notice is that training takes longer because of all the image processing. Also, your model’s accuracy\n",
    "may not be as high as it was previously, because previously it was overfitting to a largely uniform set of data. </br>\n",
    "\n",
    "In my case, when training with these augmentations my accuracy went down from 99% to 85% after 15 epochs, with validation slightly higher at 89%. (This indicates that the model is underfitting slightly, so the parameters could be tweaked a bit.) </br>\n",
    "\n",
    "As you can see, even with a relatively small dataset like Horses or Humans you can start to build a pretty decent classifier. With larger datasets you could take this further.\n",
    "Another technique to improve the model is to use features that were already learned elsewhere. Many researchers with massive resources (millions of images) and huge models that have been trained on thousands of classes have shared their models, and using a concept called transfer learning you can use the features those models learned and apply them to your data. We’ll explore that next! </br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a49b9a61aa7c46c99cc6037dd3219c6862141e82e3b9728bbf08a45545bdf65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
