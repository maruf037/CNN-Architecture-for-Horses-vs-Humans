{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN to Dintinguish Between Horses and Humans\n",
    "\n",
    "In this section we’ll explore a more complex scenario than the Fashion MNISTclassifier. We’ll extend what we’ve learned about convolutions and convolutional neural\n",
    "networks to try to classify the contents of images where the location of a feature isn’t always in the same place. I’ve created the Horses or Humans dataset for this\n",
    "purpose. </br>\n",
    "\n",
    "**The Horses vs Humans Dataset**\n",
    "\n",
    "The dataset for this section contains over a thousand 300 × 300-pixel images, approximately half each of horses and humans, rendered in different poses. You can see some examples in Figure.</br>\n",
    "\n",
    "![](Horses&Humans.png)\n",
    "\n",
    "As you can see, the subjects have different orientations and poses and the image composition varies. Consider the two horses, for example—their heads are oriented differently,\n",
    "and one is zoomed out showing the complete animal while the other is zoomed in, showing just the head and part of the body. Similarly, the humans are lit differently, have different skin tones, and are posed differently. The man has his hands on his hips, while the woman has hers outstretched. The images also contain backgrounds such as trees and beaches, so a classifier will have to determine which parts of the image are the important features that determine what makes a horse a horse and a human a human, without being affected by the background.</br>\n",
    "\n",
    "While the previous examples of predicting Y = 2X – 1 or classifying small monochrome images of clothing might have been possible with traditional coding, it’s clear that this is far more difficult, and you are crossing the line into where machine learning is essential to solve a problem.</br>\n",
    "\n",
    "An interesting side note is that these images are all computer-generated. The theory is that features spotted in a CGI image of a horse should apply to a real image. You’ll see how well this works later in this chapter.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for downloading and unzipping the training data\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()\n",
    "# End of unzip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the abve code\n",
    "This simply downloads the ZIP of the training data and unzips it into a directory at horse-or-human/training (we’ll deal with downloading the validation data shortly). This is the parent directory that will contain subdirectories for the image types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the code for CNN Architecture for Horses and Humans\n",
    "model = tf.keras.model.Sequ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of above code\n",
    "\n",
    "To use the *ImageDataGenerator* we now simply use the above code:\n",
    "\n",
    "We first create an instance of an ImageDataGenerator called train_datagen. We then specify that this will generate images for the training process by flowing them from a\n",
    "directory. The directory is training_dir, as specified earlier. We also indicate some hyperparameters about the data, such as the target size—in this case the images are\n",
    "300 × 300, and the class mode is binary. The mode is usually binary if there are just two types of images (as in this case) or categorical if there are more than two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture for Horses and Humans\n",
    "\n",
    "There are several major differences between this dataset and the Fashion MNIST onethat you have to take into account when designing an architecture for classifying the\n",
    "images. First, the images are much larger—300 × 300 pixels—so more layers may be needed. Second, the images are full color, not grayscale, so each image will have three\n",
    "channels instead of one. Third, there are only two image types, so we have a binary classifier that can be implemented using just a single output neuron, where it\n",
    "approaches 0 for one class and 1 for the other. Keep these considerations in mind when exploring this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the above code\n",
    "There are a number of things to note here. First of all, this is the very first layer. We’re defining 16 filters, each 3 × 3, but the input shape of the image is (300, 300, 3). Remember that this is because our input image is 300 × 300 and it’s in color, so there are three channels, instead of just one for the monochrome Fashion MNIST dataset\n",
    "we were using earlier. </br>\n",
    "\n",
    "At the other end, notice that there’s only one neuron in the output layer. This is because we’re using a binary classifier, and we can get a binary classification with just\n",
    "a single neuron if we activate it with a sigmoid function. The purpose of the sigmoid function is to drive one set of values toward 0 and the other toward 1, which is perfect\n",
    "for binary classification. </br>\n",
    "\n",
    "Next, notice how we stack several more convolutional layers. We do this because our image source is quite large, and we want, over time, to have many smaller images,\n",
    "each with features highlighted. If we take a look at the results of model.summary we’ll see this in action:\n",
    "\n",
    "```\n",
    "Found 1027 images belonging to 2 classes.\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
    "                                                                 \n",
    " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
    "                                                                 \n",
    " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 3136)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 512)               1606144   \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 1)                 513       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 1,704,097\n",
    "Trainable params: 1,704,097\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "Note how, by the time the data has gone through all the convolutional and pooling layers, it ends up as 7 × 7 items. The theory is that these will be activated feature maps\n",
    "that are relatively simple, containing just 49 pixels. These feature maps can then be passed to the dense neural network to match them to the appropriate labels. </br>\n",
    "\n",
    "This, of course, leads us to have many more parameters than the previous network, so it will be slower to train. With this architecture, we’re going to learn 1.7 million\n",
    "parameters. </br>\n",
    "\n",
    "To train the network, we’ll have to compile it with a loss function and an optimizer. In this case the loss function can be binary cross entropy loss function binary cross\n",
    "entropy, because there are only two classes, and as the name suggests this is a loss function that is designed for that scenario. And we can try a new optimizer, root mean\n",
    "square propagation (RMSprop), that takes a learning rate (lr) parameter that allows us to tweak the learning. Here’s the code: </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Epoch 1/15\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.7955 - accuracy: 0.5901\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.4327 - accuracy: 0.8286\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.2816 - accuracy: 0.9260\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.1887 - accuracy: 0.9552\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.1358 - accuracy: 0.9649\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.0909 - accuracy: 0.9698\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.0755 - accuracy: 0.9834\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.1505 - accuracy: 0.9737\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0343 - accuracy: 0.9893\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 5.3613e-04 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 43s 1s/step - loss: 0.2908 - accuracy: 0.9776\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 41s 1s/step - loss: 0.1162 - accuracy: 0.9776\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 2.6936e-04 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 42s 1s/step - loss: 8.7605e-05 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 41s 1s/step - loss: 0.4179 - accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "#This is the code for downloading and unzipping the training data\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "# file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"D:\\ML Codes\\AI and Machine Learning for Coders\\Horses or Human Dataset/training\"\n",
    "# urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "# zip_ref.extractall(training_dir)\n",
    "# zip_ref.close()\n",
    "#End of unzip code\n",
    "\n",
    "# This is the image generator code\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size = (300, 300),\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# This is the CNN architecture for Horses and Humans\n",
    "model = tf.keras.models.Sequential([        \n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model with loss fuction\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the model using fil_generator and \n",
    "# passing it the training_generator we created erlier.\n",
    "history = model.fit(train_generator, epochs=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the above code:\n",
    "This sample will work in Colab, but if you want to run it on your own machine,please ensure that the Pillow libraries are installed using ```pip install pillow```.</br>\n",
    "\n",
    "Also in my case I needed to install scipy, which stands for Scientific Python. It provides more utility functions for optimization, stats and signal processing. use ```pip install scipy``` to install the package.</br>\n",
    "\n",
    "Note that you can't use ```model.fit_generator``` in the latest versions of tensorflow. It is deprecated. Instead you must use ```model.fit``` option.</br>\n",
    "\n",
    "Also in order to use RMSprop as optimizer, there should be few changes to be made in the latest versions of tensorflow. You have to use ```tf.keras.optimizers.RMSprop``` syntax along with ```learning_rate``` parameter as ```lr``` is deprecated in latest versions. </br>\n",
    "\n",
    "Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy on the training set. Of course, this is just with the training data, and isn’t an indication of\n",
    "performance on data that the network hasn’t previously seen. </br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a49b9a61aa7c46c99cc6037dd3219c6862141e82e3b9728bbf08a45545bdf65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
